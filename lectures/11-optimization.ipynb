{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "\n",
    "# if we are running Colab we want to download the repository\n",
    "if \"google.colab\" in sys.modules:\n",
    "  try:\n",
    "    if 'BioChemistry' in os.environ.get('COLAB_NOTEBOOK_ID', ''):\n",
    "      content='BioChemistry'\n",
    "      try:\n",
    "        !git clone https://github.com/luchem/BioChemistry.git --depth=1\n",
    "      except Exception as e:\n",
    "        print('the loading failed, if the files already exist this will happen and can be ignored')\n",
    "        print(e)\n",
    "    else:\n",
    "      raise\n",
    "  except:\n",
    "    try:\n",
    "      content='Kemm30'\n",
    "      !git clone https://github.com/luchem/Kemm30.git --depth=1\n",
    "    except Exception as e:\n",
    "      print('the loading failed, if the files already exist this will happen and can be ignored')\n",
    "      print(e)\n",
    "  path_to_files = os.sep.join([os.getcwd(), content, \"lectures\", \"Data\"])\n",
    "else:\n",
    "  path_to_files = os.sep.join([os.getcwd(), \"Data\"])\n",
    "\n",
    "print('check that the files are here:  ' + path_to_files)\n",
    "%matplotlib inline\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repetition Functions:\n",
    "\n",
    "Two ways to define functions: \n",
    "* the classical way using def \n",
    "* a more compact way is using a so-called lambda function that you use when you only want to define a function for one purpose (here optimizing)\n",
    "\n",
    "## Definition syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_one(s):\n",
    "    \"\"\"Function that returns s plus 1\"\"\"\n",
    "    return s + 1\n",
    "\n",
    "def adding_one_with_standard(s=5):\n",
    "    \"\"\"Function that returns s plus 1 and with a default value\"\"\"\n",
    "    return s + 1\n",
    "\n",
    "print(adding_one(5))\n",
    "print(adding_one_with_standard(5))\n",
    "print(adding_one_with_standard())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have learned before functions are so called namespaces in which we can re-use names we used outside. (remember _import this_ ?) <br>\n",
    "this also means if you want to use something out of a function you have to return it.\n",
    "\n",
    "## Task:\n",
    "\n",
    "1. Write a function that takes `x`, `mu` and `sigma` and returns a gaussian bell curve (with normalisation). Make sure that you use numpy operations only, so that you can give it a vector and receive a vector.<br> \n",
    "${\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}\\operatorname {exp} \\left(-{\\frac {\\left(x-\\mu \\right)^{2}}{2\\sigma ^{2}}}\\right)$<br>\n",
    "(You should of course feel free to re-use your prior work)\n",
    "2. define `mu=0` , `sigma=1` as standard inputs\n",
    "3. create an x-vector (outside the function) from -5 to 5 in 0.01 steps and use the function to calculate the corresponding values of the gaussian \n",
    "4. use the cumsum function from numpy to create the cummulative sum of the gaussian from above and normalize it with your lambda function\n",
    "5. Put both y-vectors into a DataFrame with x as the index\n",
    "6. Plot the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss(x, mu, sigma):\n",
    "    \"\"\"task: define this function\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimisation in science\n",
    "<div>\n",
    "<img src=\"Data/KEMM30_007.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "How do you get there? or better,how do you get there without producing a lot of nonsense\n",
    "Counting parameter. e.g. 10 peaks, each position, width, intensity =30 parameter plus background. So fitting is about intelligence. Think, $\\textbf{optimize}$ the smallest amount of parameter starting with a good guess\n",
    "\n",
    "One parameter optimisation (middle = mu)\n",
    "<div>\n",
    "<img src=\"Data/error_way.png\" width=\"1000\">\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"Data/KEMM30_008.jpg\" width=\"700\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter optimisation: Fitting vs. optimisation\n",
    "\n",
    "In science as in any research we try to make statements based upon our data. For this we need to extract these numbers from our data. To do this in a standardized way we \"refine\" a model function to our data.\n",
    "\n",
    "In this course we will focus on three related methods to do that. In the advanced tasks below we will briefly mention a few more. The implementations of refining we will be looking into are:\n",
    "\n",
    "1. **Curve Fit**<br>\n",
    "   Curve fit is intended for exactly what the name says, fitting a single curve to data. Important is that the parameters are non dependent on each other. So think fitting a line, or a simple polynom.\n",
    "2. **Scipy minimize**<br>\n",
    "    Scipy minimize is a powerful function for optimization and the standard. It can use many minimization functions. It requires that we write a cost function. Parameter are handled as lists. \n",
    "3. **LMfit**\n",
    "    Is pretty much an \"add-on\" to scipy minimize. It has a number of special features, but we will focus on its parameter handling. As for scipy minimize you have to write a cost function. Parameter are however handled as dictionary type objects.\n",
    "\n",
    "\n",
    "## Curve Fit\n",
    "Starting with `curve_fit`. We assume that we have a flat function that has a clear gradient to the minimum (see top of this page) then we can use `curve_fit` to fast measure the parameter we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay(x, I0=1, lifetime=1):\n",
    "    \"\"\" Exponential decay function \"\"\"\n",
    "    return I0 * np.exp(-x / lifetime)\n",
    "\n",
    "# create some data with some randomness and plot it\n",
    "x = np.linspace(0, 5, 200)\n",
    "y = decay(x, I0=0.9, lifetime=1.1)\n",
    "y += 0.5 * y * np.random.random(np.shape(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a guess (starting values)\n",
    "p0 = [0.8, 0.9]  # in order as in function\n",
    "\n",
    "# optimise\n",
    "popt, pcov = curve_fit(decay, xdata=x, ydata=y, p0=p0)\n",
    "\n",
    "# plot both\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, \"o\", label=\"data\")\n",
    "ax.plot(x, decay(x, I0=p0[0], lifetime=p0[1]), \"b-\", label=\"guess\")\n",
    "ax.plot(x, decay(x, I0=popt[0], lifetime=popt[1]), \"r-\", label=\"fit\")\n",
    "ax.legend()\n",
    "\n",
    "# get errors from the covariance matrix (works here, but careful)\n",
    "perr = np.sqrt(np.diag(pcov))\n",
    "df = pd.DataFrame({\"values\": popt, \"errors\": perr}, index=[\"I0\", \"lifetime\"])\n",
    "ax.text(x=2, y=0.4, s=df.to_string());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key bit\n",
    "```python\n",
    "from scipy.optimize import curve_fit\n",
    "popt,pcov = curve_fit(decay, xdata=x, ydata=y,p0=p0)\n",
    "```\n",
    "curve fit is a least square method that takes a function, the target data and a set of starting parameters, that are in order the parameter after the first.\n",
    "it returns: \n",
    "`popt = optimized` parameter\n",
    "and\n",
    "`pcov = covariance` matrix.\n",
    "`p_sigma = np.sqrt(np.diag(pcov))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"Data/KEMM30_009.jpg\" width=\"800\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:\n",
    "\n",
    "1. Read the file `fit_0.csv`\n",
    "2. Fit the peak in the file\n",
    "3. plot both, data and fits:\n",
    "4. What is the center position of the peak?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy minimize\n",
    "\n",
    "Second way of optimisation uses in addition to the \"cost function\" an \"error function\". The task of the second function is to create a \"price\" for this parameter. The Minimize function is then minimizing this price.\n",
    "\n",
    "### Parameter unpacking\n",
    "\n",
    "Note how `*p` _unpacks_ the list into the positional function arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine gauss to work with parameter vector p=mu,sigma,offset,scale\n",
    "def gauss(x, mu=0, sigma=1, offset=0, scale=1):\n",
    "    prefactor = 1 / np.sqrt(2 * np.pi * sigma**2)\n",
    "    e_function = np.exp((-0.5 / sigma**2) * np.subtract(x, mu)**2)\n",
    "    return prefactor * scale * e_function + offset\n",
    "\n",
    "\n",
    "# create some data\n",
    "x = np.linspace(-5, 5, 200)\n",
    "p = [0.5, 0.5, 0, 1]\n",
    "y = gauss(x, *p)\n",
    "y += y * 0.5 * np.random.random(np.shape(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new thing is that you need a second function that produces you a single \"error\" value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_gauss(p, x, y):\n",
    "    \"\"\"Root mean square function\"\"\"\n",
    "    return np.sqrt(\n",
    "        ((y - gauss(x, *p))**2).sum()\n",
    "    )\n",
    "\n",
    "def min_gauss_lin(p, x, y):\n",
    "    \"\"\"\n",
    "    This is a different cost function that uses a more linear approach.\n",
    "    It does not \"punish\" strong deviations as much. As such it has the\n",
    "    tendency to be more outlier stable but preforms bad for peaks.\n",
    "    \"\"\"\n",
    "    return np.abs(y - gauss(x, *p)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = [0.6, 0.7, 0.1, 0.9]\n",
    "\n",
    "# nelder-Mead is a standard multi-parameter optimiser. check out other choices:\n",
    "out = minimize(min_gauss, x0=x0, args=(x, y), method=\"Nelder-Mead\")\n",
    "out2 = minimize(min_gauss_lin, x0=x0, args=(x, y), method=\"Nelder-Mead\")\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare the x vector and the x0 vector to see that the optimization has worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.plot(x, y, \"o\", ms=5, label=\"data\")\n",
    "ax.set_xlabel(\"measured value\")\n",
    "ax.set_ylabel(\"occurance\")\n",
    "ax.plot(x, gauss(x, *x0), color=\"blue\", lw=2, alpha=0.5, label=\"start\")\n",
    "ax.plot(x, gauss(x, *out[\"x\"]), color=\"red\", linestyle=\"dashed\", lw=2, label=\"fit_sqrt\")\n",
    "ax.plot(x, gauss(x, *out2[\"x\"]), color=\"green\", lw=2, zorder=0, label=\"fit_lin\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions with dictionaries for parameter handling\n",
    "\n",
    "While it is very useful to hand lists of parameters to a function it is very difficult to keep track of what parameter is where, \n",
    "what it does and when it is updated. A good habbit is to define parameter with dictionaries or Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_with_names(x, par):  # the function that is your model\n",
    "    \"\"\"\n",
    "    Gaussian type Peak function.\n",
    "\n",
    "    Parameters:\n",
    "    sigma and mu are mandatory, scale and offset are optional\n",
    "    \"\"\"\n",
    "\n",
    "    pre_factor = 1 / np.sqrt(2 * np.pi * (par[\"sigma\"]**2))\n",
    "    exponent = (-0.5 / par[\"sigma\"] ** 2) * (x - par[\"mu\"])**2\n",
    "\n",
    "    if \"scale\" in list(par.keys()):\n",
    "        pre_factor *= par[\"scale\"]\n",
    "    if \"offset\" in list(par.keys()):\n",
    "        return pre_factor * np.exp(exponent) + par[\"offset\"]\n",
    "    else:\n",
    "        return pre_factor * np.exp(exponent)\n",
    "\n",
    "def min_gauss(par, x, y):\n",
    "    \"\"\"The root mean square (we skip the root as the minimum is the minimum)\"\"\"\n",
    "    return np.sqrt(((y - gauss_with_names(x, par)) ** 2).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LMFIT\n",
    "\n",
    "While scipy minimize is a very useful tool, it is still a bit difficult to handle parameters (counting positions can produce errors). A very nice tool that was developed for this purpose is lmfit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmfit\n",
    "\n",
    "# first create a parameter object\n",
    "par = lmfit.Parameters()  # create empty parameter object\n",
    "\n",
    "par.add(\"mu\", value=0.4, vary=True)  # Add a parameter\n",
    "par.add(\"sigma\", value=0.5, vary=True)  # Add a parameter\n",
    "par.add(\"scale\", value=1, vary=True)  # Add a parameter\n",
    "\n",
    "mini = lmfit.Minimizer(min_gauss, par, fcn_kws={\"x\": x, \"y\": y})  # Add a parameter\n",
    "results = mini.minimize(\"nelder\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "\n",
    "1. Read the files\n",
    "2. Fit files using either minimize or lmfit (or both if you want to train)\n",
    "3. plot both: data and fits:\n",
    "\n",
    "* `fit_0.csv`\n",
    "* `fit_1.csv`  here: try first a separate fit, in which you fit the linear range and then separately the peak.\n",
    "* `fit_3.csv` (two peaks and background)\n",
    "\n",
    "# Advanced\n",
    "## Additional Modules to look into\n",
    "\n",
    "1. **Statistical tools**<br>\n",
    "    1. `scipy.stats` While there are a lot of statistical tools available in pandas and numpy, there are also some specialized tools for that.  scipy.stats offers you a wide range of tools for testing and correlation analysis        \n",
    "    2. `statsmodels` offers a central API very similar to R and is pretty much the portation of R to python\n",
    "2. **Numbers, constants, errors** \n",
    "    1. `scipy.constants` <br>\n",
    "    Numpy might contain some of the most used constants. this database however is specialized on providing all potential values.\n",
    "    2. `uncertainties`<br>\n",
    "    is a package that allows the calculation with numbers including error values. This tool takes care of all the error propagation. \n",
    "\n",
    "3. **Biopython**<br>\n",
    "    is a specialized suite for biological calculations and datahandling.\n",
    "\n",
    "4. GUI<br>\n",
    "    1. build in is TKinter. This is a standard platform that makes GUI maybe not nice, but easy\n",
    "    2. QT is certainly the most powerful GUI generation tool, but not always the easiest\n",
    "    3. pythonwx is a useful package\n",
    "    \n",
    "There is an endless number of very powerful calculation tools for pretty much any purpose. Before starting on a new I highly recommend to check how professional it is done. A good indication are extensive documentation, example use cases and very limited requirements (dependencies). \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
