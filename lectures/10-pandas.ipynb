{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!git clone https://github.com/luchem/Kemm30.git --depth=1\n",
    "path_to_files=os.sep.join([os.getcwd(),'Kemm30','lectures','Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make operating with various data more comfortable a number of special libraries have been developed. the maybe most important and quasy standard is Pandas. \n",
    "Pandas is a big library designed for structured data, with statistical tools, convenient data importing functions and tools to filter and extract data.\n",
    "\n",
    "A whole bunch of tutorials for Pandas are available online:\n",
    "https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html<br>\n",
    "or<br>\n",
    "https://dataanalysispython.readthedocs.io/en/latest/index.html<br>\n",
    "The latter beeing my personal favorite<br>\n",
    "\n",
    "We will focus on 4 functions of pandas:\n",
    "1. The convenient way to slice by names\n",
    "2. The very simple but powerful plot function\n",
    "3. The import function, that has options for most types of data\n",
    "4. Very fast binning of categorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = {\n",
    "    'Compound': ['Water', 'Ethanol', 'Benzene', 'Methanol', 'Acetone', 'Toluene', 'Tetrachloroethylene'],\n",
    "    'MolecularWeight': [18.015, 46.07, 78.1134, 32.042, 58.08, 92.14, 165.8],\n",
    "    'BoilingPoint': [100, 78.37, 80.1, 64.7, 56.08, 110.6, 121.2],\n",
    "    'Solubility': ['Infinite', 'Miscible', '0.178 g/100 mL', 'Miscible', 'Miscible', 'None', '206 mg/L']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.index=df.loc[:,'Compound']   # assign the column Compound as new index\n",
    "df.drop('Compound',axis=1,inplace=True)      # drop the column Compound.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DataFrames are a mix between arrayes and dictionaries, which is also the easiest way to generate them.\n",
    "* So we assign the column Compound as new index\n",
    "* drop the column Compound\n",
    "    * The option axis=1 is needed as the default is to look into the rows\n",
    "    * The option inplace=True is needed as the function would otherwise return a copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have already used one of the great advantages of Pandas, the name based slicing using the \"locator\"<br>\n",
    "df.loc[\"name of row\",\"name of column\"]<br>\n",
    "As before we can use the \":\" to represent \"from beginning until\" or \"from here until\", or simply \"all\"<br>\n",
    "and now we can calulate with this. So to calculate the weight of one millimol of the solvent we can calculate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:,'MolecularWeight']*1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or filter data. If the column name does not contain any dots or spaces a lazy version is:<br>\n",
    "df.loc[:,column_name] = df.column_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.BoilingPoint>100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of slicing but becomes really useful for spectroscopic data where one or both of the indexes are actually numbers. Because then the indexes allow the slicing of regions.<br> To illustrate this we use the very convenient plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0,4,0.05)*np.pi\n",
    "df=pd.DataFrame(np.sin(x),index=x)\n",
    "plt.close('all')\n",
    "fig,ax=plt.subplots()\n",
    "df.plot(ax=ax,style='*')\n",
    "df[1:3]+=0.2\n",
    "df.plot(ax=ax)\n",
    "ax.legend(['original','altered'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice that the slicing finds the closest value to the given cutoff (the value x=3 does not exist)<br>\n",
    "Also in this slicing have I used the \"lazy version\" of slicing. The clean version would have been:<br>\n",
    "df.loc[1:3,:]\n",
    "\n",
    "Pandas has as numpy also the possibility to slice after location with the **iloc** locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10:13] # row 10 - 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames (df) are inherently two dimensional objects that have \n",
    "\n",
    "* rows defined by the dfindex. (think x-axis)\n",
    "* columns defined by the df.columns\n",
    "* the indexes as a whole have a name (think x-axis label) in df.index.name\n",
    "* the columns as a whole have a name (think y-axis label) in df.columns.name\n",
    "* and the whole DataFrame has a name in df.name\n",
    "\n",
    "while this is a bit of typing, the power of this becomes clear when plotting, or saving/loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timen=np.arange(0,60,0.1)   # create a time vector\n",
    "dicten={}                   # create an empty dictionary to contain the data\n",
    "for rate in np.arange(1.5,10,1):     # loop over the parameter you want to use and put the current parameter into \"rate\"\n",
    "    y=np.exp(-timen/rate)            # create the vector with the y-values \n",
    "    dicten['%.1f'%rate]=y            # store the value in the dictionary with the parameter as the key \n",
    "df=pd.DataFrame(dicten,index=timen)              # Now create the dataframe. \n",
    "df.index.name='time [s]'             # give the x-axis a name\n",
    "df.columns.name='rate [mol/s]'       # give the parameter a name\n",
    "df.name='Concentation'\n",
    "plt.close('all')\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "internally everything is just a numpy array  that can be access by \"values\".<br>\n",
    "the index and columns can be given as a list/vector of names <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array=df.values\n",
    "index_array=df.index.values\n",
    "columns_array=df.columns.values\n",
    "#print(data_array,index_array,columns_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we have beyond the usual statistical tools\n",
    "\n",
    "``` py\n",
    "    df.min()\n",
    "    df.max()\n",
    "    df.mean()\n",
    "    df.var()\n",
    "```\n",
    "\n",
    "also more advanced statistical tools such as \n",
    "\n",
    "``` py\n",
    "    df.describe()\n",
    "```\n",
    "\n",
    "## Task: \n",
    "generate a dataframe that has an index one column with an index from 0-1s, with one column containing an sin(2*pi*time/50), one column with the square of this function one column with the absolute value. Use the describe function to determine the average power that can be extracted from this powerline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that can sometimes create confusion is that a DataFrame consists of columns that are inherently single vectors that are called \"Series\" and have the dimension 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df),'has dimensions',df.ndim)\n",
    "print(type(df.iloc[:,1]),'has dimensions',df.iloc[:,1].ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data with Pandas\n",
    "The import function of Pandas is one of the most powerful features. It can scrape data from the web and adopt to pretty much any format found with just small adjustments. Its usage is very similar to any data handling in python with the same challenges:\n",
    "\n",
    "* find the data\n",
    "* tell the program what the shape of the file is\n",
    "* handle problems (if there are any)\n",
    "\n",
    "We will simplify the problem by again downloading the repository and all the files you need will be in the folder \"Data\" If you have done so in the first cell or in a previous notebook in the same session, this will give you an error that the folder exits... which you can happily ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!git clone https://github.com/luchem/Kemm30.git --depth=1\n",
    "path_to_files=os.sep.join([os.getcwd(),'Kemm30','lectures','Data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data in **sinc.dat** using the function df=pd.read_csv() \n",
    "\n",
    "For this you can either open the file directly from the internet or download it into your working directory first. \n",
    "\n",
    "The open the file and note (maybe on a paper) \n",
    "\n",
    "1. if there is any text before the data that you do not want to read\n",
    "2. if there are any headers (text before the data that tells what the columns are)\n",
    "3. what are the separators between the numbers\n",
    "4. which column (if any) would be good to use as index (think x-axis in plot)\n",
    "\n",
    "Play with the options until you get the\n",
    "\n",
    "in the index. the options filename (the internet adress), sep (the separator) and index_col (which column to use for the index) are the ones that you need to take a good look at. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked and you named you imported dataframe *df* <br>\n",
    "it should look like this if you call \n",
    "\n",
    "```\n",
    "df.head()\n",
    "```\n",
    "\n",
    "<div>\n",
    "<img src=\"http://www.jensuhlig.de/Kemm30/sinc_reading.jpg\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "now try what \n",
    "\n",
    "``` py\n",
    "df.plot() \n",
    "```\n",
    "\n",
    "gives you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you convert the index and the columns to numbers, sorting makes more sense. Compare the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read=df.copy() #to make a deep (real) copy\n",
    "df_read.columns=df_read.columns.values.astype(float)\n",
    "df_read.index=df_read.index.values.astype(float)\n",
    "df_read.sort_index(inplace=True,axis='columns')\n",
    "plt.close('all')\n",
    "df_read.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly it is a good idea to give both columns and rows a name, which is then also used during plotting. The y-scale label you typically have to write by hand (tomorrow). Plot and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.index.name='x-value'\n",
    "df_read.columns.name='parameter'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce the same plot for the file **sinc_2.dat** in the data folder The first row are the headers and should be read! if the data looks strange follow the stepwise check from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "\n",
    "The data file **APS_Copper_SolarCell.dat** was measured at a large scale research facility: the \"Advanced light source\" in Chicago and represents X-ray absorption data. Ignore the first 30 rows (and header) and read all columns from the file. Hint the separator \"\\s+\" separate for one or multiple white spaces.\n",
    "\n",
    "Use copy paste to extract the column names (Row 30) from the text file and paste it in your Notebook as a long string. Then use the string method \"split\" to separate the string into a list of column names\n",
    "\n",
    "give this list of names to the use the \"names\" parameter to give the columns in the DataFrame the right name. \n",
    "\n",
    "select the index column by the name \"Energy\" instead of the number.\n",
    "\n",
    "Choose to only read the column \"PR\" (column 14)\n",
    "\n",
    "If you have done the selection during the import then plotting the column \"PR\" vs \"Energy\" is simply\n",
    "df_read.plot() \n",
    "\n",
    "If you import all columns, but want to select only one to plot you would use: df[\"PR\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked as expected the plot should look like this:\n",
    "<div>\n",
    "<img src=\"http://www.jensuhlig.de/Kemm30/APS_reading.jpg\" width=\"300\">\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation, this is most likely one of the most difficult files you ever need to read, combining all the techniques you have learned up to now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting and grouping\n",
    "This is the last main function of huge data package that is pandas.<br>\n",
    "In general the idea is that data that i categorized can be grouped together and e.g. summarized or statistically analyzed. We will be looking at two different problems. \n",
    "\n",
    "1. First will use this function to efficiently rebin data\n",
    "2. then we will analyze real some inherently categorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a simple dataset with a lot of noise \n",
    "\n",
    "time=np.arange(0,4,0.001)   # create a time vector\n",
    "y=np.sin(2*np.pi*time)+np.random.rand(time.shape[0])\n",
    "dicten={'time':time,'y':y}                   # create an empty dictionary to contain the data\n",
    "df=pd.DataFrame(dicten,index=time)              # Now create the dataframe. \n",
    "plt.close('all')\n",
    "fig,ax=plt.subplots()\n",
    "df.y.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added the time axis to the dataframe (and the index) now we need to generate a category=a set where the data has a same feature. Here simple round the time data with one digit after the comma. After this, the column time (but not the index are in groups and we can use the function **groupby** to collect the same values together. We go then one step further and take the average of the values in the group.  \n",
    "## Task\n",
    "\n",
    "```py\n",
    "df.time=df.time.round(decimals=1)\n",
    "df.groupby('time').mean()\n",
    "```\n",
    "1. investigate what this code does before and after the grouping.\n",
    "2. plot both the original df and the df after the grouping and mean() into the same graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig,ax=plt.subplots()\n",
    "df.plot(ax=ax)\n",
    "df.time=df.time.round(decimals=1)\n",
    "df.groupby('time').mean().plot(ax=ax)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic sampling (real trial data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the data and set the filepath\n",
    "\n",
    "!git clone https://github.com/luchem/Kemm30.git --depth=1\n",
    "path_to_files=os.sep.join([os.getcwd(),'Kemm30','lectures','Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(path_to_files + os.sep +'data_from_trial.xlsx',sheet_name='inner')\n",
    "df['replica']=[a.split('.')[-1] for a in df.object.values]\n",
    "df['treatmentA']=[a[0] for a in df.object.values]\n",
    "df['treatmentB']=[a[1] for a in df.object.values]\n",
    "plt.close('all')\n",
    "ax=df.boxplot(by=['treatmentA','treatmentB'],layout=(1,3),figsize=(10,8),rot=90,whis=[5,95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these categorie the sample could like in the previous example be grouped into the different treatments. \n",
    "```\n",
    "df.groupby(['treatmentA','treatmentB'])\n",
    "```\n",
    "Can you see significant differences? What does a \"significant\" difference mean? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manual solution to column reading would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names='N  Epoch  Energy  k  Mono  Seconds  ion1  ion2  ion3  ion4  mud  mud1  mostabc  SYNM  PR  bun-1  bun-1GS  bun1  bun1GS  bun2  bun2GS  DifSYN  Difb_1  Difb1  Difb2  c0o0b0  c0o1b0  c0o2b0  c0o3b0  c0o4b0  c0o5b0  c0o6b0  c0o7b0  c0o8b0  c0o9b0  c0o10b0  c0o11b0  c0o12b0  c0o13b0  c0o14b0  c0o15b0  c0o16b0  c0o17b0  c0o18b0  c0o19b0  c0o20b0  c0o21b0  c1o0b0  c1o1b0  c1o2b0  c1o3b0  c1o4b0  c1o5b0  c1o6b0  c1o7b0  c1o8b0  c1o9b0  c1o10b0  c1o11b0  c1o12b0  c1o13b0  c1o14b0  c1o15b0  c1o16b0  c1o17b0  c1o18b0  c1o19b0  c1o20b0  c1o21b0  c2o0b0  c2o1b0  c2o2b0  c2o3b0  c2o4b0  c2o5b0  c2o6b0  c2o7b0  c2o8b0  c2o9b0  c2o10b0  c2o11b0  c2o12b0  c2o13b0  c2o14b0  c2o15b0  c2o16b0  c2o17b0  c2o18b0  c2o19b0  c2o20b0  c2o21b0  c3o0b0  c3o1b0  c3o2b0  c3o3b0  c3o4b0  c3o5b0  c3o6b0  c3o7b0  c3o8b0  c3o9b0  c3o10b0  c3o11b0  c3o12b0  c3o13b0  c3o14b0  c3o15b0  c3o16b0  c3o17b0  c3o18b0  c3o19b0  c3o20b0  c3o21b0'\n",
    "col_names=col_names.split()\n",
    "print('print the first 8 entries: {}'.format(col_names[:8]))\n",
    "files=os.sep.join([os.getcwd(),'Kemm30','lectures','Data','APS_Copper_SolarCell.dat'])\n",
    "df3=pd.read_csv(files,skiprows=30,sep='\\s+',names=col_names,index_col='Energy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are automatic ways how to do this column extraction:\n",
    "The first method only reads the columns you want and reduces the work, but requires me to count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3a=pd.read_csv(files,skiprows=30,sep='\\s+',names=['Energy','PR'],index_col=0,usecols=[2,14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method uses an escape character to extract the column names automatically and allows then the use of the name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3b=pd.read_csv(files,skiprows=29,escapechar='L',sep='\\s+',index_col=0,usecols=['Energy','PR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Task: \n",
    "\n",
    "Create the a DataFrame with position from -5 cm to 5 cm as the index and in the columns different gaussians bell curves <br>\n",
    "${\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}\\operatorname {exp} \\left(-{\\frac {\\left(x-\\mu \\right)^{2}}{2\\sigma ^{2}}}\\right)$<br>\n",
    "with the same central position (mu=0) and different width sigma (0.5,1,2,3,4)\n",
    "using the simplified plotting from above to show them in the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise you _manually_ extracted data from the internet which is problematic due to several reasons:\n",
    "\n",
    "1. it's time consuming\n",
    "2. it's error prone\n",
    "3. what if the source is updated / corrected?\n",
    "\n",
    "The following example uses _Pandas_ which is an external module used to handle large data bases; millions of entries is not a problem. As you will see, we simple point it to the Wikipedia page and it will automatically - and almost magically - detect the table and extract the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tables = pd.read_html(\"https://en.wikipedia.org/wiki/Hydrophobicity_scales\")\n",
    "p = tables[0] # list of table found. Only one is found on the page. \n",
    "p.head() # show the first five rows (the head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.to_excel('hydrophobicity.xlsx') # save to MS Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we \"zip\" up two columns to form a dictionary. In Pandas, index can be given by their names.\n",
    "d = dict( zip( p['Amino acid'], p['Interface scale, ΔGwif (kcal/mol)'] ) )\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['Cys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "delta_G_values = list(d.values())\n",
    "delta_G_values = [float(i.replace('−', '-')) for i in delta_G_values] # weird minus sign on wikipedia!\n",
    "aminoacid_names = list(d.keys())\n",
    "plt.bar(aminoacid_names, delta_G_values) # bar plot expects x and y values as lists\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('amino acid')\n",
    "plt.ylabel('$\\Delta G$ (kcal/mol)')\n",
    "plt.title('source: wikipedia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Exercise\n",
    "\n",
    "In the above we created two lists `delta_G_values` and `aminoacid_names` but the data is taken directly from the Wikipedia table and unsorted with respect to $\\Delta G$. Use the answer to [this question](https://stackoverflow.com/questions/9764298/is-it-possible-to-sort-two-listswhich-reference-each-other-in-the-exact-same-w) to simultaneously sort both lists and re-plot the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "\n",
    "1. Extract all tables from Wikipedias [list of Nobel laureates in Chemistry](https://en.wikipedia.org/wiki/List_of_Nobel_laureates_in_Chemistry) into a list named `nobel`\n",
    "1. How many tables did you find?\n",
    "1. Show and investigate the first table\n",
    "1. Try the following code:\n",
    "~~~ py\n",
    "# https://stackoverflow.com/questions/40581312/how-to-create-a-frequency-table-in-pandas-python\n",
    "df = pd.value_counts(nobel[0]['Country[B]']).to_frame().reset_index()\n",
    "mask = df['Country[B]']>1 # only countries with two or more prizes\n",
    "df = df[mask]\n",
    "explode = df['index']=='Sweden'\n",
    "plt.pie( df['Country[B]'].values, labels=df['index'], autopct='%1.0f%%', radius=1.5, explode=explode )\n",
    "plt.show()\n",
    "~~~\n",
    "1. have a look at `df`.\n",
    "1. have a look at `mask`. How does it work?\n",
    "1. have a look at `explode`. How does it work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "url     = 'https://en.wikipedia.org/wiki/List_of_Nobel_laureates_in_Chemistry'\n",
    "nobel   = pd.read_html(url)\n",
    "df      = pd.value_counts(nobel[0]['Country[B]']).to_frame().reset_index()\n",
    "mask    = df['Country[B]']>1 # only countries with two or more prizes\n",
    "df      = df[mask]\n",
    "explode = df['index']=='Sweden'\n",
    "plt.pie(df['Country[B]'].values,\n",
    "        labels=df['index'],\n",
    "        autopct='%1.0f%%',\n",
    "        radius=1., \n",
    "        explode=explode )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
