{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will familiarize us with Pandas, the todays de-facto library for data handling. Pandas is a big library designed on top of numpy for structured data, with statistical tools, convenient data importing functions and tools to filter and extract data.\n",
    "\n",
    "A whole bunch of tutorials for Pandas are available online:\n",
    "https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html<br>\n",
    "or<br>\n",
    "https://dataanalysispython.readthedocs.io/en/latest/index.html<br>\n",
    "The latter beeing my personal favorite<br>\n",
    "\n",
    "We will focus on a few functions of pandas:\n",
    "1. The convenient way to organise and slice data\n",
    "2. The import function, that has options for most types of data and will work nearly always\n",
    "3. The very simple but powerful plot function\n",
    "4. Grouping, sorting data and statistical tool\n",
    "\n",
    "Pandas has three Basic structures:\n",
    "* Series, 1 dimensional data with an index\n",
    "* 2D data with index and columns\n",
    "* DateTime Frame 2D dataframe with specialized index\n",
    "\n",
    "If you slice a 2d Dataframe into a single column you have a pandas Series. All have a somewhat similar structure.\n",
    "\n",
    "A DataFrame has the structure:\n",
    "<div>\n",
    "<img src=\"Data/DataFrameStructure.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "One of the most important differences to e.g. arrays is that you can select/slice the data by name. Lets create a notebook and take a look\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#create dictionary with data\n",
    "data = {\n",
    "    'Compound': ['Water', 'Ethanol', 'Benzene', 'Methanol', 'Acetone', 'Toluene', 'Tetrachloroethylene'],\n",
    "    'MolecularWeight': [18.015, 46.07, 78.1134, 32.042, 58.08, 92.14, 165.8],\n",
    "    'BoilingPoint': [100, 78.37, 80.1, 64.7, 56.08, 110.6, 121.2],\n",
    "    'Solubility': ['miscible', 'miscible', 'slightly miscible', 'miscible', 'miscible', 'not miscible', 'slightly miscible']\n",
    "}\n",
    "\n",
    "#create a DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "df=df.set_index('Compound') # make a specifric column the index\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see are notebooks very well readible in jupyter. First we used print(df) and it still looks good. As it had initially indexes as row names we exchanged this with a specific column. \n",
    "\n",
    "For the looks:<br>\n",
    "There are a lot of conversion options, including excel and markdown \n",
    "\n",
    "\n",
    "## Task\n",
    "\n",
    "print a markdown table from this dataframe, hint check df.to_ < tab > and print the resulting string. Looks familiar?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting, slicing and plotting\n",
    "\n",
    "Like in arrays we can slice using indexes and booleans. But DataFrames have several other great selection possibilities that is related to their similarity to dictionaries. Which is name/value based slicing.\n",
    "\n",
    "This selection can be done as part of the value in the cells using e.g. \"groupby\" or by the indexes, on which we will exclusively focus. (see advanced part for grouping, that is very useful for separate treatments)\n",
    "\n",
    "## Name - values based slicing\n",
    "\n",
    "For a DataFrame like above we can use the **name** of a columns to select a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BoilingPoint']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or multiple names as a list or vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['BoilingPoint','MolecularWeight']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The better and more structure way to **\"locate\"** a certain value/region in the DataFrame is to use the \"name base locator\" df.loc[\"name of row\",\"name of column\"]<br> \n",
    "In a series (a single column) we do of course only need the row index.<br>\n",
    "If the index or columns we are selecting from are numbers and sorted this allows some really convenient tricks that show some of the usefullness of this.\n",
    "\n",
    "The following line reads a stored file and sets the first column as index.<br> \n",
    "we then use the build in plotting function to look at the read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Data/sinc.dat',index_col=0)\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to inspect the contents of the file one can:\n",
    "* print it by typing **df**\n",
    "* print the first x (default=5) elements with **df.head()**\n",
    "* print the last x (default=5) elements with **df.tail()**\n",
    "* look at the descriptive summery with **df.describe()**\n",
    "\n",
    "But here we want to slice. So we use df.loc[] and as you would with indexes slices. But the values i give are the values from the axis. What is really neat is that these values do not really need to exist. Pandas will find the nearest value and use that instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[-1:1,:].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to properly work the entries in the index need to be numbers. So we can check the \"type\" of the  entries and if needed convert them to number with the command \"astype\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index # look at the last line. dtype=float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice the difference? The values are actually strings and the type is 'object' which means something mixed or non regular.<br>\n",
    "\n",
    "So we replace the columns with the same values, but now converted into numbers and while we are on it sort them after size. Now we can also slice the second column. (note that the values 6 and 12 are not in the list but the closest has been selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=df.columns.values.astype(float)\n",
    "df.sort_index(inplace=True,axis='columns')\n",
    "df.loc[-1:1,6:12].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would of course have been very useful for doing calculations on sections of data.\n",
    "\n",
    "As the example above shows can commands be \"stacked\". Here we first sliced and then plotted. Important to note is that we have not assigned this temporarily sliced notebook to anything. so we have not written\n",
    "\n",
    "    df=df.loc[-1:1,6:12]\n",
    "    df.plot()\n",
    "\n",
    "The difference is important, as we have not changed df, only create a local copy. We will later use this trick to stack a number of commands. But here this creates a convenient way to make plots. To take full advantage we should assign a name to the columns and the index as a whole.<br> compare the plots to see the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.name='function x-value in cm'\n",
    "df.columns.name=r'parameter for the $\\frac{sin(x)}{n * x}$ calculation'\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(8,4))\n",
    "df.loc[-1:1,6:12].plot(ax=ax,legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "While this was kind of useful for categorical data, but becomes really useful for e.g. spectroscopic data<br>\n",
    "Here we read in the measured absorption of silicon, please plot this spectrum only in the visible spectral range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"Data/Silicon_Absorption.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Indexes and values\n",
    "Both indexes and values in the main matrix are essentially numpy vectors/arrays and can be extracted. \n",
    "\n",
    "    data_array=df.values\n",
    "    index_array=df.index.values\n",
    "    columns_array=df.columns.values\n",
    "\n",
    "or set with a list&vector of the same shape:\n",
    "\n",
    "    df.columns=['Hey I'm column 1','Hey I'm column 2','Hey I'm column 3' ]\n",
    "    \n",
    "Indexing (setting or changing of indexes) is one of the big challenges in working with Pandas and will take some time to get used to. However they can have some really cool features as the following example from the **biochemistry** world will show.\n",
    "\n",
    "First we create two lists with 600000 measured sequences of each 3 or 128 basepairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_list1=np.random.choice(['A','B','C','D'],size=(3,600000))\n",
    "testing_list1=np.apply_along_axis(''.join, 0, testing_list1)\n",
    "\n",
    "testing_list2=np.random.choice(['A','B','C','D'],size=(3,600000))\n",
    "testing_list2=np.apply_along_axis(''.join, 0, testing_list2)\n",
    "\n",
    "print(testing_list1.shape)\n",
    "testing_list1[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* then we count how often we have each item in the list using Counter that creates a dictionary with the name and how often the item is in it.\n",
    "* we then create a pandas series for each\n",
    "* and calculate the difference between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "starting_time=time.time()\n",
    "\n",
    "#count how often is each basepair in sequence\n",
    "intens1=Counter(testing_list1)\n",
    "intens2=Counter(testing_list2)\n",
    "\n",
    "#create series from counts\n",
    "series1=pd.Series(intens1,index=intens1.keys())\n",
    "series2=pd.Series(intens2,index=intens2.keys())\n",
    "\n",
    "intermediary_time=time.time()\n",
    "\n",
    "#calculate how different are they (in the sum)\n",
    "diff=series1.subtract(series2, fill_value=0, axis=0)\n",
    "difference=diff[diff.abs()>0].abs().sum()\n",
    "\n",
    "final_time=time.time()\n",
    "print('%g basepairs were different calculated in %.3g ms for %g samples of %g bases'%(difference,(final_time-starting_time)*1000,600000,3))\n",
    "print('from this time only %.3g ms were spend to joining the base-pairs using a pandas series'%((final_time-intermediary_time)*1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the feature of DataFrames/Series to merge two completly different matrixes. If e.g. one of the keys (here base pairs) does not exist in one of the series it is intelligently added. <br>\n",
    "<br> The uses are endless, from procedures in chemistry over samples in testing or to the here intelligent comparision.\n",
    "\n",
    "## Index based and boolean slicing\n",
    "\n",
    "Of course we have the same methods as numpy. \n",
    "with \n",
    "\n",
    "    df.iloc[2:4,1:5]\n",
    "    \n",
    "we perform an index based slicing. The difference, we always slice the indexes as well as the data.<br>\n",
    "This means if in the example of the SINC function from above you slice the x-axis (e.g. spectral axis) and the parameter axis. One would do that in numpy or Matlab with:\n",
    "\n",
    "    array=array[2:4,1:5]\n",
    "    x_axis=x_axis[2:4]\n",
    "    y_axis=y_axis[1:5]\n",
    "    \n",
    "The same is true for bolean (value) type slicing\n",
    "\n",
    "    df[df>5] = 0\n",
    "    \n",
    "Will for example set all values in the matrix where the intensity is bigger than 5 to 0. We will practice this one we have learned to read Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data with Pandas\n",
    "\n",
    "reading arbitrary data from a file or the web can be very tedious and difficult. Here we want to use the power of Pandas to simplify this process\n",
    "\n",
    "Essentially anything that you do _manually_ when extracted data from the internet which is problematic due to several reasons:\n",
    "\n",
    "1. it's time consuming\n",
    "2. it's error prone\n",
    "3. what if the source is updated / corrected?\n",
    "\n",
    "We have already used Pandas to read files above\n",
    "\n",
    "    df=pd.read_csv(\"Data/Silicon_Absorption.csv\",index_col=0)\n",
    "    \n",
    "Which nearly magically recognizes the shape of the file and reads it. We will train more later. Pandas is a powerful tool for reading and handling data. It can easily handle large data bases; millions to billions of entries. It even can read even larger files bunch by bunch. \n",
    "\n",
    "Additionally we can simply point it to the Wikipedia page and it will automatically - and almost magically - detect tables and extract the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(\"https://en.wikipedia.org/wiki/Hydrophobicity_scales\")\n",
    "p = tables[0] # list of table found. Only one is found on the page. \n",
    "p.head() # show the first five rows (the head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.set_index('Amino acid',inplace=True) # we want to use aminoacids as index\n",
    "p=p['Interface scale, ΔGwif (kcal/mol)']  # and only want one of the columns\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenges when reading data\n",
    "\n",
    "Notice the **dtype:object** again? \n",
    "turns out Wikipedia uses a weired type of minus signs.\n",
    "if you try:\n",
    "\n",
    "    p.astype(float)\n",
    "\n",
    "it fails. Wikipedia as well as a few machines made in India (Chinese ones usually work) use a different type of minus sign. After replacing the strange minus with a proper minus we can again build the pandas series and now create very nice plots. <br> One thing of note. In my 15 years of using python I have only a handful of times needed to do something like in this example. Each of which used Wikipedia, which was why we wanted to show you the trick.\n",
    "\n",
    "Starting again by reading and shaping, then in Line 6 we replace the minus sign, after which we can plot (or do some calculations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(\"https://en.wikipedia.org/wiki/Hydrophobicity_scales\")\n",
    "p = tables[0] # list of table found. Only one is found on the page. \n",
    "p.set_index('Amino acid',inplace=True) # we want to use aminoacids as index\n",
    "p=p['Interface scale, ΔGwif (kcal/mol)']  # and only want one of the columns\n",
    "\n",
    "p=pd.Series([float(s.replace('−', '-')) for s in p.values],index=p.index,name=p.name)\n",
    "fig,ax=plt.subplots()\n",
    "ax=p.plot(ax=ax,kind='bar')\n",
    "ax.set_ylabel(p.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This summarizes the 4 typical challenges when handling data.**\n",
    "\n",
    "1. Finding the files/plots/sources\n",
    "2. Telling the program where they are\n",
    "3. Telling the program how to read the data\n",
    "4. Cleaning up the data you have read.\n",
    "\n",
    "\n",
    "## Task\n",
    "\n",
    "Read the data in \" http://www.jensuhlig.de/Kemm30/sinc.dat \" or \"Data/sinc.dat\" using the function df=pd.read_csv() \n",
    "\n",
    "For this you can either open the file directly from the internet or download it into your working directory first. \n",
    "\n",
    "In general when working with text files: Important **DO NOT** double click but open the file in a better plain text reader. I recommend something like Notepad++ . Look at the file and note (maybe on a paper) \n",
    "\n",
    "1. if there is any text before the data that you do not want to read\n",
    "2. if there are any headers (text before the data that tells what the columns are)\n",
    "3. what are the separators between the numbers\n",
    "4. which column (if any) would be good to use as index (think x-axis in plot)\n",
    "\n",
    "These 4 points translate into the 4 most used options in pd.read_csv() \n",
    "\n",
    "    1. skip_rows= 0 or x\n",
    "    2. header=None or True\n",
    "    3. sep=',' or '\\t' (tab) or '\\s+' one or multiple white\n",
    "    4. index_col = 0 or x or None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked it should look like this if\n",
    "\n",
    "<div>\n",
    "<img src=\"Data/sinc_reading.jpg\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce the same plot for \"http://www.jensuhlig.de/Kemm30/sinc_2.dat\" or \"Data/sinc_2.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful:Dealing with directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to load and handle many datafiles we should quickly look into how to handle directories. \n",
    "Loading a file from a sub directory needs the filesystem separator, meaning the sign that separates folders and files.\n",
    "\n",
    "If you are working on \"binder\" or \"garm\" then the program of your Jupyter notebook is running on the server. This means that you need to upload your data to this server to be able to use it.\n",
    "\n",
    "Lets assume that you have your datafiles in a (relative) subfolder with the name \"Data\" Then during file import you need to replace you filename \"sinc.dat\" with the path to the file<br>\n",
    "under windows this means using the backslash \"data\\sinc.dat\"<br>\n",
    "as however the \"\\\" is a special character we need to use \"\\\\\" to create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='data\\\\sinc.dat' #under windows\n",
    "filename='data/sinc.dat' #under MAC and Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library \"os\" provides some simple tools to make this more comfortable <br>\n",
    "\"os.sep\" provides the platform independent separator (on linux and windows alike) <br>\n",
    "\"os.getcwd()\" read \"get current working directory\" can tell you where you currently are<br>\n",
    "For more complicated path there is the excellent library \"pathlib\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #import the library\n",
    "filename='data' + os.sep + 'sinc.dat'\n",
    "filename=os.sep.join(['data','sinc.dat']) #easy to read and independent of which platform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can also use the features of \"jupyter lab\" to copy the path<br> \n",
    "or finally use a graphical interface to select your file. See this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path():\n",
    "    from tkinter import filedialog\n",
    "    import tkinter\n",
    "    root_window = tkinter.Tk()\t\t\t\n",
    "    root_window.withdraw()\n",
    "    root_window.attributes('-topmost',True)\n",
    "    root_window.after(1000, lambda: root_window.focus_force())\n",
    "    complete_path = filedialog.askopenfilename(initialdir=os.getcwd())\n",
    "    return complete_path\n",
    "if 0: #so that we can run \"run all above\"\n",
    "    get_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "The data file \" http://www.jensuhlig.de/Kemm30/APS_Copper_SolarCell.dat \" \"Data/APS_Copper_SolarCell.dat\" was measured at a large scale research facility: the \"Advanced light source\" in Chicago and represents X-ray absorption data. Ignore the first 30 rows (and header) and read all columns from the file. We want to plot the Column \"PR\" vs the Column \"Energy\"\n",
    "\n",
    "If everything worked it should look like this:\n",
    "<div>\n",
    "<img src=\"http://www.jensuhlig.de/Kemm30/APS_reading.jpg\" width=\"300\">\n",
    "</div> \n",
    "\n",
    "Now you have multiple choices how to make this work.\n",
    "\n",
    "1. count the number of rows and slice. The you can either plot by hand or create a pd.Series for the plot.\n",
    "2. copy Row 30 from the text file and paste it in your Notebook as a long string. Then use the string method \"split\" to separate the string into a list of column names and replace the automatic column names with these names during the import\n",
    "3. import the columns and replace after the import the column names. use the set_index we have used prior\n",
    "4. learn more of the many options, there are always some combinations that lead to success.\n",
    "\n",
    "\n",
    "\n",
    "Congratulation, this is most likely one of the most difficult files you ever need to read, combining all the techniques you have learned up to now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries are often the easiest way to create and name DataFrames other methods are attaching columns to dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timen=np.arange(0,60,0.1)   # create a time vector\n",
    "dicten={}                   # create an empty dictionary to contain the data\n",
    "for rate in np.arange(1.5,10,1):     # loop over the parameter you want to use and put the current parameter into \"rate\"\n",
    "    y=np.exp(-timen/rate)            # create the vector with the y-values \n",
    "    dicten['%.1f'%rate]=y            # store the value in the dictionary with the parameter as the key \n",
    "df=pd.DataFrame(dicten)              # Now create the dataframe. \n",
    "df.index.name='time [s]'             # give the x-axis a name\n",
    "df.columns.name='rate [mol/s]'       # give the parameter a name\n",
    "df.name='Concentation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "1. Create a DataFrame with the index in from of floats from -5 cm to 5 cm in 0.05 steps.\n",
    "2. in the columns add different gaussians bell curves <br>\n",
    "    ${\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}\\operatorname {exp} \\left(-{\\frac {\\left(x-\\mu \\right)^{2}}{2\\sigma ^{2}}}\\right)$<br>\n",
    "with the same central position (mu=0) but different width sigma (0.5,1,2,3,4)\n",
    "3. use the different sigma as column names\n",
    "4. using the simplified plotting from above to show them in the same plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics with DataFrames\n",
    "\n",
    "you have already learnt **max**, **min**, **std** **sum**, **mean**, **median**, **describe**. All of these functions can be applied on all or per row, per column,...\n",
    "\n",
    "A very useful tool can be the groupby functions. Lets look on our original data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'Compound': ['Water', 'Ethanol', 'Benzene', 'Methanol', 'Acetone', 'Toluene', 'Tetrachloroethylene'],\n",
    "    'MolecularWeight': [18.015, 46.07, 78.1134, 32.042, 58.08, 92.14, 165.8],\n",
    "    'BoilingPoint': [100, 78.37, 80.1, 64.7, 56.08, 110.6, 121.2],\n",
    "    'Solubility': ['miscible', 'miscible', 'slightly miscible', 'miscible', 'miscible', 'not miscible', 'slightly miscible']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df=df.set_index('Compound') # make a specifric column the index\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can create groups by any categories. e.g. here the entries in Solubility\n",
    "groups=df.groupby('Solubility')\n",
    "#Then we can perform calculations within the groups.\n",
    "groups['BoilingPoint'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "Calculate the average molecular weight for the groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: RDKit\n",
    "\n",
    "**Obs!** you will need to install RDKit ([här](https://www.rdkit.org/docs/Install.html))<br>\n",
    "you can do that from within this notebook with \n",
    "    \n",
    "    !pip install \n",
    "    \n",
    "and then restart your kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "m = Chem.MolFromSmiles('c1ccccc1C(=O)O')\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of atoms:\", m.GetNumAtoms())\n",
    "for atom in m.GetAtoms():\n",
    "    print(atom.GetSymbol())\n",
    "print(\"number of atoms with hydrogen:\", Chem.AddHs(m).GetNumAtoms())\n",
    "print(\"Molar mass:\",Descriptors.MolWt(m),\"g/mol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rdkit.Chem import PandasTools\n",
    "\n",
    "mollist = []\n",
    "mollist.append(Chem.MolFromSmiles('O'))\n",
    "mollist.append(Chem.MolFromSmiles('CO'))\n",
    "mollist.append(Chem.MolFromSmiles('CCO'))\n",
    "mollist.append(Chem.MolFromSmiles('CC(=O)C'))\n",
    "mollist.append(Chem.MolFromSmiles('c1ccccc1'))\n",
    "mollist.append(Chem.MolFromSmiles('c1ccccc1C'))\n",
    "mollist.append(Chem.MolFromSmiles('ClC(Cl)C(Cl)Cl'))\n",
    "df['Structure'] = mollist\n",
    "\n",
    "PandasTools.RenderImagesInAllDataFrames(images=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solution to the automatic reading\n",
    "df3b=pd.read_csv(\"http://www.jensuhlig.de/Kemm30/APS_Copper_SolarCell.dat\",skiprows=29,escapechar='L',sep='\\s+',index_col=0,usecols=['Energy','PR'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
