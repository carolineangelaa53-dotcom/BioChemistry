{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# if we are running Colab we want to download the repository\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    path_to_files = os.sep.join([os.getcwd(), \"Kemm30\", \"lectures\", \"Data\"])\n",
    "    !git clone https://github.com/luchem/Kemm30.git --depth=1\n",
    "else:\n",
    "    path_to_files = os.sep.join([os.getcwd() , \"Data\"])\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make operating with various data more comfortable a number of special libraries have been developed. The maybe most important and quasy standard is _Pandas_. Pandas is a big library designed for structured data, with statistical tools, convenient data importing functions and tools to filter and extract data.\n",
    "\n",
    "A whole bunch of tutorials for Pandas are available online:\n",
    "\n",
    "- A [series of video lectures](https://youtube.com/playlist?list=PLto3nNV9nKZlXSWOAqmmn4J7csD4I6a2d&si=MXH5re0I73LW9VdI) from a Lund University graduate course (numpy, matplotlib, and pandas).\n",
    "- https://pandas.pydata.org/docs/getting_started/intro_tutorials/index.html\n",
    "- https://dataanalysispython.readthedocs.io/en/latest/index.html (Jens' personal favorite)\n",
    "\n",
    "We will focus on 4 functions of pandas:\n",
    "1. The convenient way to slice by names\n",
    "2. The very simple but powerful plot function\n",
    "3. The import function, that has options for most types of data\n",
    "4. Very fast binning of categorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Compound\": [\n",
    "        \"Water\",\n",
    "        \"Ethanol\",\n",
    "        \"Benzene\",\n",
    "        \"Methanol\",\n",
    "        \"Acetone\",\n",
    "        \"Toluene\",\n",
    "        \"Tetrachloroethylene\",\n",
    "    ],\n",
    "    \"MolecularWeight\": [18.015, 46.07, 78.1134, 32.042, 58.08, 92.14, 165.8],\n",
    "    \"BoilingPoint\": [100, 78.37, 80.1, 64.7, 56.08, 110.6, 121.2],\n",
    "    \"Solubility\": [\n",
    "        \"Infinite\",\n",
    "        \"Miscible\",\n",
    "        \"0.178 g/100 mL\",\n",
    "        \"Miscible\",\n",
    "        \"Miscible\",\n",
    "        \"None\",\n",
    "        \"206 mg/L\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.index = df.loc[:, \"Compound\"]  # assign the column Compound as new index\n",
    "df.drop(\"Compound\", axis=1, inplace=True)  # drop the column Compound.\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DataFrames are a mix between arrayes and dictionaries, which is also the easiest way to generate them.\n",
    "* So we assign the column Compound as new index\n",
    "* drop the column Compound\n",
    "    * The option `axis=1` is needed as the default is to look into the rows\n",
    "    * The option `inplace=True` is needed as the function would otherwise return a copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have already used one of the great advantages of Pandas, the name based slicing using the \"locator\"<br>\n",
    "`df.loc[\"name of row\",\"name of column\"]`<br>\n",
    "As before we can use the `\":\"` to represent \"from beginning until\" or \"from here until\", or simply \"all\"<br>\n",
    "and now we can calulate with this. So to calculate the weight of one millimol of the solvent we can calculate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"MolecularWeight\"] * 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or filter data. If the column name does not contain any dots or spaces a lazy version is:<br>\n",
    "`df.loc[:,column_name] = df.column_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.BoilingPoint > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of slicing but becomes really useful for spectroscopic data where one or both of the indexes are actually numbers. Because then the indexes allow the slicing of regions.<br> To illustrate this we use the very convenient plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 4, 0.05) * np.pi\n",
    "df = pd.DataFrame(np.sin(x), index=x)\n",
    "plt.close(\"all\")\n",
    "fig, ax = plt.subplots()\n",
    "df.plot(ax=ax, style=\"*\")\n",
    "df[1:3] += 0.2\n",
    "df.plot(ax=ax)\n",
    "ax.legend([\"original\", \"altered\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you notice that the slicing finds the closest value to the given cutoff (the value x=3 does not exist)<br>\n",
    "Also in this slicing have I used the \"lazy version\" of slicing. The clean version would have been:<br>\n",
    "`df.loc[1:3,:]`\n",
    "\n",
    "Pandas has as numpy also the possibility to slice after location with the `iloc` locator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[10:13]  # row 10 - 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames (df) are inherently two dimensional objects that have \n",
    "\n",
    "* rows defined by the dfindex. (think x-axis)\n",
    "* columns defined by the df.columns\n",
    "* the indexes as a whole have a name (think x-axis label) in df.index.name\n",
    "* the columns as a whole have a name (think y-axis label) in df.columns.name\n",
    "* and the whole DataFrame has a name in df.name\n",
    "\n",
    "while this is a bit of typing, the power of this becomes clear when plotting, or saving/loading files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timen = np.arange(0, 60, 0.1)  # create a time vector\n",
    "dicten = {}  # create an empty dictionary to contain the data\n",
    "\n",
    "# loop over the parameter you want to use and\n",
    "# put the current parameter into \"rate\"\n",
    "for rate in np.arange(1.5, 10, 1):\n",
    "    y = np.exp(-timen / rate)  # create the vector with the y-values\n",
    "    dicten[f\"{rate}\"] = (\n",
    "        y  # store the value in the dictionary with the parameter as the key\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(dicten, index=timen)  # Now create the dataframe.\n",
    "df.index.name = \"time [s]\"  # give the x-axis a name\n",
    "df.columns.name = \"rate [mol/s]\"  # give the parameter a name\n",
    "df.name = \"Concentation\"\n",
    "plt.close(\"all\")\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "internally everything is just a numpy array  that can be access by \"values\".<br>\n",
    "the index and columns can be given as a list/vector of names <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array = df.values\n",
    "index_array = df.index.values\n",
    "columns_array = df.columns.values\n",
    "# print(data_array,index_array,columns_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we have beyond the usual statistical tools\n",
    "\n",
    "```python\n",
    "df.min()\n",
    "df.max()\n",
    "df.mean()\n",
    "df.var()\n",
    "```\n",
    "\n",
    "also more advanced statistical tools such as \n",
    "\n",
    "```python\n",
    "df.describe()\n",
    "```\n",
    "\n",
    "## Task: \n",
    "generate a dataframe that has an index one column with an index from 0-1s, with one column containing an sin(2*pi*time/50), one column with the square of this function one column with the absolute value. Use the describe function to determine the average power that can be extracted from this powerline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that can sometimes create confusion is that a DataFrame consists of columns that are inherently single vectors that are called \"Series\" and have the dimension 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df), \"has dimensions\", df.ndim)\n",
    "print(type(df.iloc[:, 1]), \"has dimensions\", df.iloc[:, 1].ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data with Pandas\n",
    "The import function of Pandas is one of the most powerful features. It can scrape data from the web and adopt to pretty much any format found with just small adjustments. Its usage is very similar to any data handling in python with the same challenges:\n",
    "\n",
    "* find the data\n",
    "* tell the program what the shape of the file is\n",
    "* handle problems (if there are any)\n",
    "\n",
    "We will simplify the problem by again downloading the repository and all the files you need will be in the folder \"Data\" If you have done so in the first cell or in a previous notebook in the same session, this will give you an error that the folder exits... which you can happily ignore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data in *THIATS_EtOH_simple.TXT* using the function `df=pd.read_csv()`.\n",
    "\n",
    "For this you can either open the file directly from the internet or download it into your working directory first. \n",
    "\n",
    "The open the file and note (maybe on a paper) \n",
    "\n",
    "1. if there is any useless text before the data that you do not want to read<br>\n",
    "    skiprows=0 (default)\n",
    "2. if there are any headers (text before the data that tells what the columns are)<br>\n",
    "    header=True (Default)\n",
    "3. what are the separators between the numbers. The typical choices are:\n",
    "    * comma (default)         sep=','\n",
    "    * tab                     sep='\\t'\n",
    "    * one or multiple spaces  sep='\\s+'\n",
    "4. which column (if any) would be good to use as index (think x-axis in plot)<br>\n",
    "    index_col=0   (or a name if you have names)\n",
    "\n",
    "Play with the options until you get in the index. the options filename, sep (the separator) and index_col (which column to use for the index) are the ones that you need to take a good look at. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "read the file `sinc.dat`\n",
    "If everything worked and you named you imported dataframe *df* <br>\n",
    "it should look like this if you call \n",
    "\n",
    "```\n",
    "df.head()\n",
    "```\n",
    "\n",
    "<div>\n",
    "<img src=\"Data/sinc_reading.jpg\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "now try what \n",
    "\n",
    "``` py\n",
    "df.plot() \n",
    "```\n",
    "\n",
    "gives you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you convert the index and the columns to numbers, sorting makes more sense. Compare the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = df.copy()  # to make a deep (real) copy\n",
    "df_read.columns = df_read.columns.values.astype(float)\n",
    "df_read.index = df_read.index.values.astype(float)\n",
    "df_read.sort_index(inplace=True, axis=\"columns\")\n",
    "plt.close(\"all\")\n",
    "df_read.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly it is a good idea to give both columns and rows a name, which is then also used during plotting. The y-scale label you typically have to write by hand (tomorrow). Plot and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read.index.name = \"x-value\"\n",
    "df_read.columns.name = \"parameter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce the same plot for the file `sinc_2.dat` in the data folder The first row are the headers and should be read! if the data looks strange follow the stepwise check from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Reading and processing. The data in file `THIATS_EtOH.TXT` was measured with a photospectrometer and is data very similar to what you need to process during the \"wet lab\" in the course KEMM30.\n",
    "\n",
    "1. Read it into a pandas series with the wavelength as index and plot it\n",
    "2. Now convert the wavelength axis into electron energy by using the formula $E [eV] = \\frac{h \\cdot c}{e \\cdot \\lambda}$ <br>\n",
    "   where `c` is the speed of light,<br>\n",
    "   `h` the Planck constant <br>\n",
    "   `e` the elementary charge<br>\n",
    "   \"$\\lambda$\" the wavelength in m <br>\n",
    "   take the value of these constants from the library `scipy.constants` <br>\n",
    "   name the new index \"energy in eV\"<br>\n",
    "   verify with `df.idxmax()` (assuming that your series is called df) that the maximum of the new axis is at 2.246 eV\n",
    "4. Sort the index \n",
    "5. Plot the new curve\n",
    "6. Sum the intensity under the curve between 2eV and 2.8eV and multiply it with the width between two x-points in the middle of the peak (0.00805) <br> hint if the slice between 2 and 2.8 is empty (or the sum = 0) then your sorting has not worked.\n",
    "7. From `scipy.integrate` use the `trapezoid` rule and calculate the integral<br> Hint when you take the data out of the dataframe using the squeeze() function makes sure that you get a nice vector of a single dimension. <br> Don't forget that you need both x and y and that the function might take the two in a different order\n",
    "8. The difference between the two methods is about 5 percent\n",
    "\n",
    "   x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "The data file `APS_Copper_SolarCell.dat` was measured at a large scale research facility: the \"Advanced light source\" in Chicago and represents X-ray absorption data. Ignore the first 30 rows (and header) and read all columns from the file. Hint the separator `\"\\s+\"` separate for one or multiple white spaces.\n",
    "\n",
    "Use copy paste to extract the column names (Row 30) from the text file and paste it in your Notebook as a long string. Then use the string method \"split\" to separate the string into a list of column names\n",
    "\n",
    "give this list of names to the use the \"names\" parameter to give the columns in the DataFrame the right name. \n",
    "\n",
    "select the index column by the name \"Energy\" instead of the number.\n",
    "\n",
    "Choose to only read the column \"PR\" (column 14)\n",
    "\n",
    "If you have done the selection during the import then plotting the column \"PR\" vs \"Energy\" is simply `df_read.plot()`.\n",
    "\n",
    "If you import all columns, but want to select only one to plot you would use: `df[\"PR\"].plot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything worked as expected the plot should look like this:\n",
    "<div>\n",
    "<img src=\"Data/APS_reading.jpg\" width=\"300\">\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation, this is most likely one of the most difficult files you ever need to read, combining all the techniques you have learned up to now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sorting and grouping\n",
    "This is the last main function of the huge data package that is pandas.<br>\n",
    "In general the idea is that data that i categorized can be grouped together and e.g. summarized or statistically analyzed. We will be looking at two different problems. \n",
    "\n",
    "1. First will use this function to efficiently rebin data\n",
    "2. then we will analyze real some inherently categorized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a simple dataset with a lot of noise\n",
    "\n",
    "time = np.arange(0, 4, 0.001)  # create a time vector\n",
    "y = np.sin(2 * np.pi * time) + np.random.rand(time.shape[0])\n",
    "dicten = {\"time\": time, \"y\": y}  # create an empty dictionary to contain the data\n",
    "df = pd.DataFrame(dicten, index=time)  # Now create the dataframe.\n",
    "plt.close(\"all\")\n",
    "fig, ax = plt.subplots()\n",
    "df.y.plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added the time axis to the dataframe (and the index) now we need to generate a category=a set where the data has a same feature. Here simple round the time data with one digit after the comma. After this, the column time (but not the index are in groups and we can use the function `groupby()` to collect the same values together. We go then one step further and take the average of the values in the group.\n",
    "\n",
    "## Task\n",
    "\n",
    "```python\n",
    "df.time=df.time.round(decimals=1)\n",
    "df.groupby('time').mean()\n",
    "```\n",
    "1. investigate what this code does before and after the grouping.\n",
    "2. plot both the original df and the df after the grouping and mean() into the same graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "fig, ax = plt.subplots()\n",
    "df.plot(ax=ax)\n",
    "df.time = df.time.round(decimals=1)\n",
    "df.groupby(\"time\").mean().plot(ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic sampling (real trial data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(path_to_files + os.sep + \"data_from_trial.xlsx\", sheet_name=\"inner\")\n",
    "\n",
    "df[\"replica\"] = [a.split(\".\")[-1] for a in df.object.values]\n",
    "df[\"treatmentA\"] = [a[0] for a in df.object.values]\n",
    "df[\"treatmentB\"] = [a[1] for a in df.object.values]\n",
    "\n",
    "plt.close(\"all\")\n",
    "ax = df.boxplot(\n",
    "    by=[\"treatmentA\", \"treatmentB\"],\n",
    "    layout=(1, 3),\n",
    "    figsize=(10, 8),\n",
    "    rot=90,\n",
    "    whis=[5, 95],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these categorie the sample could like in the previous example be grouped into the different treatments. \n",
    "```python\n",
    "df.groupby(['treatmentA','treatmentB'])\n",
    "```\n",
    "Can you see significant differences? What does a \"significant\" difference mean? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: \n",
    "\n",
    "investigate the plot and understand what it shows. Look (print) what the DataFrame contains. Can you do some calculations with there groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical plotting with Pandas and Seaborn\n",
    "\n",
    "Matplotlib is very powerfull library and you can in principle create any plot you want. Typically one generates a style of plot once and then reuses the same plotting functions (your own). However particularly for statistical plotting there have been developed a number of modules that do that for you. **seaborn** is a widely used plotting tool, that also chooses nice colors. The plotting commands are a bit more difficult, but produce informative plots. Here two examples.\n",
    "\n",
    "Important to note is however that seaborn *uses* matplotlib. That means that all the plotting command you learned before will work and permit you to customize the plots seaborn creates, in addition to the commands provided by seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = sns.load_dataset(\"penguins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task: \n",
    "investigate the DataFrame using describe and by printing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A plot to illustrate distributions\n",
    "\n",
    "sns.jointplot(\n",
    "    data=df,\n",
    "    x=\"bill_length_mm\", y=\"bill_depth_mm\", hue=\"species\",\n",
    "    kind=\"kde\", fill=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: \n",
    "look into the help of *sns.jointplot* and test a few of the options "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A plot with moments in it\n",
    "sns.boxenplot(x=\"species\", y=\"body_mass_g\", hue=\"sex\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task: \n",
    "look into the help of *sns.jointplot* and test a few of the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The matplotlib version is not as informative\n",
    "plt.boxplot([df[df['species'] == s]['body_mass_g'].dropna() for s in df['species'].unique()],\n",
    "            labels=df['species'].unique())\n",
    "plt.ylabel(\"Body Mass (g)\")\n",
    "plt.title(\"Matplotlib Boxplot (no extra quantiles)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a violin plot: body mass by species, split by sex, with clear styling\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.violinplot(\n",
    "    data=df, \n",
    "    x=\"species\", \n",
    "    y=\"body_mass_g\", \n",
    "    hue=\"sex\", \n",
    "    split=True,\n",
    "    inner=\"quartile\",           # Shows the quartiles inside the violin\n",
    "    palette=\"muted\"\n",
    ")\n",
    "plt.title(\"Body Mass Distribution by Species and Sex\")\n",
    "plt.ylabel(\"Body Mass (g)\")\n",
    "plt.xlabel(\"Penguin Species\")\n",
    "plt.legend(title=\"Sex\", loc=\"upper right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The manual solution to column reading would be to copy paste the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = \"\"\"\n",
    "               N  Epoch  Energy  k  Mono  Seconds  ion1  ion2  ion3  ion4  mud  mud1 \n",
    "               mostabc  SYNM PR  bun-1  bun-1GS  bun1  bun1GS  bun2  bun2GS  DifSYN\n",
    "               Difb_1  Difb1  Difb2  c0o0b0  c0o1b0  c0o2b0  c0o3b0  c0o4b0  c0o5b0 \n",
    "               c0o6b0  c0o7b0  c0o8b0  c0o9b0  c0o10b0  c0o11b0  c0o12b0  c0o13b0 \n",
    "               c0o14b0  c0o15b0  c0o16b0  c0o17b0  c0o18b0  c0o19b0  c0o20b0  c0o21b0 \n",
    "               c1o0b0  c1o1b0  c1o2b0  c1o3b0  c1o4b0  c1o5b0  c1o6b0  c1o7b0  c1o8b0 \n",
    "               c1o9b0  c1o10b0  c1o11b0  c1o12b0  c1o13b0  c1o14b0  c1o15b0  c1o16b0 \n",
    "               c1o17b0  c1o18b0  c1o19b0  c1o20b0  c1o21b0  c2o0b0  c2o1b0  c2o2b0  c2o3b0 \n",
    "               c2o4b0  c2o5b0  c2o6b0  c2o7b0  c2o8b0  c2o9b0  c2o10b0  c2o11b0  c2o12b0 \n",
    "               c2o13b0  c2o14b0  c2o15b0  c2o16b0  c2o17b0  c2o18b0  c2o19b0  c2o20b0 \n",
    "               c2o21b0  c3o0b0  c3o1b0  c3o2b0  c3o3b0  c3o4b0  c3o5b0  c3o6b0  c3o7b0 \n",
    "               c3o8b0  c3o9b0  c3o10b0  c3o11b0  c3o12b0  c3o13b0  c3o14b0  c3o15b0 \n",
    "               c3o16b0  c3o17b0  c3o18b0  c3o19b0  c3o20b0  c3o21b0\n",
    "            \"\"\".split() # split single string into list of string\n",
    "\n",
    "print(f\"print the first 8 of {len(col_names)} entries:\\n {col_names[:8]}\")\n",
    "\n",
    "files = os.sep.join(\n",
    "    [os.getcwd(), \"Kemm30\", \"lectures\", \"Data\", \"APS_Copper_SolarCell.dat\"]\n",
    ")\n",
    "\n",
    "df3 = pd.read_csv(files, skiprows=30, sep=\"\\s+\", names=col_names, index_col=\"Energy\")\n",
    "df3.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are automatic ways how to do this column extraction:\n",
    "The first method only reads the columns you want and reduces the work, but requires me to count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3a = pd.read_csv(\n",
    "    files, skiprows=30, sep=\"\\s+\", names=[\"Energy\", \"PR\"], index_col=0, usecols=[2, 14]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second method uses an escape character to extract the column names automatically and allows then the use of the name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3b = pd.read_csv(\n",
    "    files, skiprows=29, escapechar=\"L\", sep=\"\\s+\", index_col=0, usecols=[\"Energy\", \"PR\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Task: \n",
    "\n",
    "Create the a DataFrame with position from -5 cm to 5 cm as the index and in the columns different gaussians bell curves <br>\n",
    "${\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}\\operatorname {exp} \\left(-{\\frac {\\left(x-\\mu \\right)^{2}}{2\\sigma ^{2}}}\\right)$<br>\n",
    "with the same central position (mu=0) and different width sigma (0.5,1,2,3,4)\n",
    "using the simplified plotting from above to show them in the same plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise you _manually_ extracted data from the internet which is problematic due to several reasons:\n",
    "\n",
    "1. it's time consuming\n",
    "2. it's error prone\n",
    "3. what if the source is updated / corrected?\n",
    "\n",
    "The following example uses _Pandas_ which is an external module used to handle large data bases; millions of entries is not a problem. As you will see, we simple point it to the Wikipedia page and it will automatically - and almost magically - detect the table and extract the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_html(\"https://en.wikipedia.org/wiki/Hydrophobicity_scales\")\n",
    "p = tables[0]  # list of table found. Only one is found on the page.\n",
    "p.head()  # show the first five rows (the head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.to_excel(\"hydrophobicity.xlsx\")  # save to MS Excel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we \"zip\" up two columns to form a dictionary. In Pandas, index can be given by their names.\n",
    "d = dict(zip(p[\"Amino acid\"], p[\"Interface scale, ΔGwif (kcal/mol)\"]))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[\"Cys\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "delta_G_values = list(d.values())\n",
    "delta_G_values = [\n",
    "    float(i.replace(\"−\", \"-\")) for i in delta_G_values\n",
    "]  # weird minus sign on wikipedia!\n",
    "aminoacid_names = list(d.keys())\n",
    "plt.bar(aminoacid_names, delta_G_values)  # bar plot expects x and y values as lists\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"amino acid\")\n",
    "plt.ylabel(\"$\\Delta G$ (kcal/mol)\")\n",
    "plt.title(\"source: wikipedia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Exercise\n",
    "\n",
    "In the above we created two lists `delta_G_values` and `aminoacid_names` but the data is taken directly from the Wikipedia table and unsorted with respect to $\\Delta G$. Use the answer to [this question](https://stackoverflow.com/questions/9764298/is-it-possible-to-sort-two-listswhich-reference-each-other-in-the-exact-same-w) to simultaneously sort both lists and re-plot the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise\n",
    "\n",
    "1. Extract all tables from Wikipedias [list of Nobel laureates in Chemistry](https://en.wikipedia.org/wiki/List_of_Nobel_laureates_in_Chemistry) into a list named `nobel`\n",
    "1. How many tables did you find?\n",
    "1. Show and investigate the first table\n",
    "1. Try the following code:\n",
    "```python\n",
    "# Modified after https://stackoverflow.com/questions/40581312/how-to-create-a-frequency-table-in-pandas-python\n",
    "url     = 'https://en.wikipedia.org/wiki/List_of_Nobel_laureates_in_Chemistry'\n",
    "nobel   = pd.read_html(url)\n",
    "df      = pd.value_counts(nobel[0]['Country[B]']).to_frame()\n",
    "mask    = df['count']>1 # only countries with two or more prizes\n",
    "df      = df[mask]\n",
    "explode = df.index.values =='Sweden'\n",
    "plt.pie(df.values.squeeze(),\n",
    "        labels=df.index.values,\n",
    "        autopct='%1.0f%%',\n",
    "        radius=1., \n",
    "        explode=explode )\n",
    "plt.show()\n",
    "```\n",
    "5. have a look at `df`.\n",
    "6. have a look at `mask`. How does it work?\n",
    "7. have a look at `explode`. How does it work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
